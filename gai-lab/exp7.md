### 7. Mathematical Foundations of VAEs and GANs – Derive key mathematical concepts behind these architectures.

### Mathematical Foundations of Variational Autoencoders (VAEs) and Generative Adversarial Networks (GANs)

Generative models, including **Variational Autoencoders (VAEs)** and **Generative Adversarial Networks (GANs)**, have become foundational in the field of machine learning due to their ability to model complex data distributions and generate realistic data. Below, we derive and explore the key mathematical concepts behind both architectures.

---

## **1. Variational Autoencoders (VAEs)**

VAEs are a class of generative models that combine ideas from **autoencoders** and **variational inference** to model complex data distributions. VAEs are primarily used for generating new data points similar to the data in a given dataset.

### 1.1 **The Objective of VAEs**

The goal of VAEs is to learn the parameters of a probabilistic model, $p(x)$, that represents the data distribution. In a VAE, we define the **joint distribution** of the data and latent variables:

$$
p(x, z) = p(x|z) p(z)
$$

Where:

- $x$ is the data (e.g., images).
- $z$ is the latent variable (e.g., a hidden vector representing the image).
- $p(x|z)$ is the likelihood of the data given the latent variable.
- $p(z)$ is the prior distribution on the latent variables.

### 1.2 **Variational Inference**

The key idea in VAEs is to use **variational inference** to approximate the **posterior distribution** $p(z|x)$, which is intractable to compute directly. To do so, we introduce a **variational distribution** $q(z|x)$ that approximates the true posterior.

The evidence lower bound (ELBO) is used as an approximation to the **log-likelihood** $\log p(x)$, which is difficult to compute exactly. The ELBO can be written as:

$$
\log p(x) \geq \mathbb{E}_{q(z|x)}[\log p(x|z)] - D_{KL}[q(z|x) || p(z)]
$$

Where:

- $\mathbb{E}_{q(z|x)}[\log p(x|z)]$ is the expected log-likelihood under the variational distribution.
- $D_{KL}[q(z|x) || p(z)]$ is the **Kullback-Leibler (KL) divergence** between the variational distribution and the prior distribution, acting as a regularizer.

This equation can be interpreted as a trade-off:

- The first term encourages the model to reconstruct the input data accurately.
- The second term regularizes the latent space by pushing $q(z|x)$ to be close to the prior $p(z)$.

### 1.3 **The VAE Loss Function**

The objective of training a VAE is to maximize the ELBO, which can be interpreted as minimizing the following loss function:

$$
\mathcal{L}_{VAE} = -\mathbb{E}_{q(z|x)}[\log p(x|z)] + D_{KL}[q(z|x) || p(z)]
$$

Where:

- The first term is the reconstruction loss (or **negative log-likelihood**), ensuring that the model generates accurate reconstructions of the input data.
- The second term is the **KL divergence**, which regularizes the latent variable distribution to be similar to the prior $p(z)$.

### 1.4 **Reparameterization Trick**

To optimize the VAE using gradient-based methods, the model needs to compute gradients with respect to the latent variables. However, the sampling process in the posterior distribution $q(z|x)$ makes it difficult to backpropagate through the model. The **reparameterization trick** allows for the reparameterization of the latent variable $z$ as:

$$
z = \mu(x) + \sigma(x) \odot \epsilon
$$

Where:

- $\mu(x)$ and $\sigma(x)$ are the mean and standard deviation output by the encoder.
- $\epsilon \sim \mathcal{N}(0, 1)$ is random noise.
- $\odot$ represents element-wise multiplication.

This allows the gradients to flow through the latent variable $z$, making it possible to train the model using stochastic gradient descent.

---

## **2. Generative Adversarial Networks (GANs)**

GANs are another class of generative models that use an **adversarial training** framework. The core idea is to train two neural networks: a **generator** and a **discriminator**. The generator learns to generate data that mimics the real data, while the discriminator learns to distinguish between real and fake data.

### 2.1 **The GAN Framework**

Let’s define the components of GANs:

- $p_{\text{data}}(x)$: The true data distribution.
- $p_{\text{model}}(x)$: The distribution of the data generated by the generator $G$.
- $D(x)$: The discriminator, which outputs a scalar indicating whether $x$ is real (from $p_{\text{data}}$) or fake (from $p_{\text{model}}$).
- $G(z)$: The generator, which generates data based on a random input $z$, sampled from a simple distribution (e.g., uniform or normal).

### 2.2 **Adversarial Training Objective**

The generator and discriminator are trained simultaneously in a **min-max** game. The generator tries to fool the discriminator into classifying its generated data as real, while the discriminator tries to correctly classify real and fake data. The objective function for GANs is:

$$
\mathcal{L}_{\text{GAN}} = \mathbb{E}_{x \sim p_{\text{data}}(x)}[\log D(x)] + \mathbb{E}_{z \sim p_{\text{z}}(z)}[\log(1 - D(G(z)))]
$$

Where:

- The first term encourages the discriminator to correctly classify real data as real.
- The second term encourages the discriminator to classify generated data as fake.

### 2.3 **Minimax Game**

The training process can be seen as a **minimax** problem where the **generator** seeks to minimize the log loss, and the **discriminator** seeks to maximize it. The objective is to train the two networks as follows:

- **Discriminator**: Maximize $\mathbb{E}_{x \sim p_{\text{data}}(x)}[\log D(x)] + \mathbb{E}_{z \sim p_{\text{z}}(z)}[\log(1 - D(G(z)))]$.
- **Generator**: Minimize $\mathbb{E}_{z \sim p_{\text{z}}(z)}[\log(1 - D(G(z)))]$.

Alternatively, the generator’s loss can be rewritten as:

$$
\mathcal{L}_{G} = \mathbb{E}_{z \sim p_{\text{z}}(z)}[\log D(G(z))]
$$

This modified loss function is known as the **vanilla GAN** objective.

### 2.4 **Wasserstein GAN (WGAN)**

One of the main issues with GANs is **training instability** due to the vanishing gradient problem. The **Wasserstein GAN** (WGAN), introduced by **Arjovsky et al. (2017)**, modifies the GAN framework by using the **Wasserstein distance** instead of the Jensen-Shannon divergence (used in the standard GAN objective). The objective for WGAN is:

$$
\mathcal{L}_{\text{WGAN}} = \mathbb{E}_{x \sim p_{\text{data}}(x)}[D(x)] - \mathbb{E}_{z \sim p_{\text{z}}(z)}[D(G(z))]
$$

Where $D(x)$ is the **critic**, which approximates the Wasserstein distance between the real and generated distributions.

WGANs significantly improve training stability by providing more meaningful gradients, which lead to more stable and higher-quality generated samples.

---

### 3. **Comparing VAEs and GANs**

| Feature                | **VAE**                                           | **GAN**                                                          |
| ---------------------- | ------------------------------------------------- | ---------------------------------------------------------------- |
| **Training**           | Optimize ELBO (reconstruction + regularization)   | Minimax game between generator and discriminator                 |
| **Latent Space**       | Continuous, interpretable latent space            | Unconstrained, difficult to interpret                            |
| **Generative Quality** | Can produce blurry samples                        | Can produce sharper samples                                      |
| **Loss Function**      | ELBO with reconstruction + KL divergence          | Adversarial loss (generator vs. discriminator)                   |
| **Model Architecture** | Encoder (for inference), Decoder (for generation) | Generator (data generation), Discriminator (data classification) |
| **Key Challenges**     | Poor sample quality due to regularization         | Mode collapse, instability in training                           |

---

### Conclusion

Both **VAEs** and **GANs** have made significant contributions to generative modeling, and their key mathematical foundations revolve around **variational inference** and **adversarial training**, respectively. While VAEs are built on probabilistic reasoning and focus on latent variable modeling, GANs use a competitive training setup between two networks. Each approach has its strengths and weaknesses, and their effectiveness depends on the application, with VAEs often producing smoother reconstructions and GANs excelling at generating sharp and high-quality images.
